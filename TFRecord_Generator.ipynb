{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord Generation\n",
    "This is the last stage for combining image annotations and image into two seperate records (train/test) split that will be ready to be uploaded into GCloud for processing. Image augmentation, preprocessing should be done prior to using functions on this script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global Variables\n",
    "processed_annotations = []~\n",
    "train_dataset = []\n",
    "test_dataset  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def LoadAndSplitDataset(annotation_file):\n",
    "    # Flush Global Vars\n",
    "    global processed_annotations\n",
    "    global train_dataset\n",
    "    global test_dataset\n",
    "    # Open File\n",
    "    annotation_data = open(annotation_file, \"r\")\n",
    "    # Load Combined Data Annotations\n",
    "    for line in annotation_data:\n",
    "        filename, class_idx, x_min_norm, y_min_norm, x_max_norm, y_max_norm = line.rstrip().split(\" \")\n",
    "        processed_annotations.append((filename, int(class_idx), float(x_min_norm), float(y_min_norm), float(x_max_norm), float(y_max_norm)))\n",
    "        \n",
    "    annotation_data.close()\n",
    "    \n",
    "    # Shuffle and Split\n",
    "    random.shuffle(processed_annotations)\n",
    "\n",
    "    train_test_split = int(0.70 * len(processed_annotations))\n",
    "    train_dataset = processed_annotations[:train_test_split]\n",
    "    test_dataset  = processed_annotations[train_test_split:]\n",
    "\n",
    "    # Sanity Check\n",
    "    print(\"Training Data Size  : {}\\nTesting Data Size   : {}\".format(len(train_dataset), len(test_dataset)))\n",
    "    print(\"Sanity Check Status : {}\".format(len(train_dataset) + len(test_dataset) == len(processed_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadAndSplitDataset(\"/Users/Work/Documents/Conversion/data/augfin.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Utility functions for creating TFRecord data sets.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def int64_list_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def bytes_list_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def float_list_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to convert a JSON array into a single tfrecord\n",
    "# For Visdrone example, a single tfrecord will typically store data correlating to one video image.\n",
    "def create_tf_example(data_info):\n",
    "  # TODO START: Populate the following variables from your example.\n",
    "  height = data_info['height'] # Image height\n",
    "  width = data_info['width'] # Image width\n",
    "  filename = data_info['filename'].encode() # Filename of the image. Empty if image is not from file\n",
    "  encoded_image_data = data_info['encoded_image_data'] # Encoded image bytes\n",
    "  image_format = data_info['image_format'].encode() # b'jpeg' or b'png'\n",
    "\n",
    "  xmins = data_info['xmins'] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "  xmaxs = data_info['xmaxs'] # List of normalized right x coordinates in bounding box\n",
    "             # (1 per box)\n",
    "  ymins = data_info['ymins'] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "  ymaxs = data_info['ymaxs'] # List of normalized bottom y coordinates in bounding box\n",
    "             # (1 per box)\n",
    "  classes_text = data_info['classes_text'] # List of string class name of bounding box (1 per box)\n",
    "  classes = data_info['classes'] # List of integer class id of bounding box (1 per box)\n",
    "  # TODO END\n",
    "  tf_label_and_data = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image/height': int64_feature(height),\n",
    "      'image/width': int64_feature(width),\n",
    "      'image/filename': bytes_feature(filename),\n",
    "      'image/source_id': bytes_feature(filename),\n",
    "      'image/encoded': bytes_feature(encoded_image_data),\n",
    "      'image/format': bytes_feature(image_format),\n",
    "      'image/object/bbox/xmin': float_list_feature(xmins),\n",
    "      'image/object/bbox/xmax': float_list_feature(xmaxs),\n",
    "      'image/object/bbox/ymin': float_list_feature(ymins),\n",
    "      'image/object/bbox/ymax': float_list_feature(ymaxs),\n",
    "      'image/object/class/text': bytes_list_feature(classes_text),\n",
    "      'image/object/class/label': int64_list_feature(classes),\n",
    "  }))\n",
    "  return tf_label_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sanity Check\n",
    "print(\"Training Data Size  : {}\\nTesting Data Size   : {}\".format(len(train_dataset), len(test_dataset)))\n",
    "print(\"Sanity Check Status : {}\".format(len(train_dataset) + len(test_dataset) == len(processed_annotations)))\n",
    "\n",
    "def process_labels(processed_annotations, dest_file):\n",
    "    writer = tf.compat.v1.python_io.TFRecordWriter(dest_file)\n",
    "    labels = [\"Okay, Defect\"]\n",
    "    \n",
    "    for array in processed_annotations:\n",
    "        img_path = '/Users/Work/Documents/Conversion/data/datasetAug/' + array[0]\n",
    "        img = cv.imread(img_path)\n",
    "        height, width, channels = img.shape\n",
    "        image_format = array[0][-4:]\n",
    "\n",
    "        data_info = {\n",
    "            'filename': img_path,\n",
    "            'image_format': image_format,\n",
    "            'width': width, \n",
    "            'height': height\n",
    "        }\n",
    "        \n",
    "        with tf.io.gfile.GFile(img_path, 'rb') as fid:\n",
    "            encoded_image_data = fid.read()\n",
    "            data_info['encoded_image_data'] = encoded_image_data  \n",
    "        \n",
    "        filename, class_idx, x_min_norm, y_min_norm, x_max_norm, y_max_norm = array\n",
    "\n",
    "        class_idx  = [int(x) for x in [class_idx]]\n",
    "        x_min_norm = [float(x) for x in [x_min_norm]]\n",
    "        y_min_norm = [float(x) for x in [y_min_norm]]\n",
    "        x_max_norm = [float(x) for x in [x_max_norm]]\n",
    "        y_max_norm = [float(x) for x in [y_max_norm]]\n",
    "        \n",
    "        class_text = [labels[c-1].encode() for c in class_idx]\n",
    "        \n",
    "        data_info['xmins'] = x_min_norm\n",
    "        data_info['xmaxs'] = x_max_norm\n",
    "        data_info['ymins'] = y_min_norm\n",
    "        data_info['ymaxs'] = y_max_norm\n",
    "        data_info['classes_text'] = class_text\n",
    "        data_info['classes'] = class_idx\n",
    "        \n",
    "        tf_record = create_tf_example(data_info)\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-176c283168bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprocess_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/Users/Work/Documents/Conversion/train.tfrecord\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprocess_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"/Users/Work/Documents/Conversion/test.tfrecord\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-6328f503cea0>\u001b[0m in \u001b[0;36mprocess_labels\u001b[1;34m(processed_annotations, dest_file)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessed_annotations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/Work/Documents/Conversion/data/datasetAug/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mimage_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_labels(train_dataset, \"/Users/Work/Documents/Conversion/train.tfrecord\")\n",
    "process_labels(test_dataset, \"/Users/Work/Documents/Conversion/test.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_augmentation(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
